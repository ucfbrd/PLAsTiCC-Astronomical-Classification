{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Can you help make sense of the Universe?\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Set up"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Required Library"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting tsfresh\n",
      "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/2f/32/265c651f4fd70751f5ada348af0f9e322b058eddcda6a6f9bb305c8d270a/tsfresh-0.11.1-py2.py3-none-any.whl (1.2MB)\n",
      "\u001b[K    100% |████████████████████████████████| 1.2MB 11.9MB/s ta 0:00:01\n",
      "\u001b[?25hRequirement already satisfied: numpy>=1.10.4 in /anaconda3/lib/python3.6/site-packages (from tsfresh) (1.14.2)\n",
      "Requirement already satisfied: dask>=0.15.2 in /anaconda3/lib/python3.6/site-packages (from tsfresh) (0.17.2)\n",
      "Requirement already satisfied: distributed>=1.18.3 in /anaconda3/lib/python3.6/site-packages (from tsfresh) (1.21.6)\n",
      "Requirement already satisfied: pandas>=0.20.3 in /anaconda3/lib/python3.6/site-packages (from tsfresh) (0.20.3)\n",
      "Requirement already satisfied: patsy>=0.4.1 in /anaconda3/lib/python3.6/site-packages (from tsfresh) (0.5.0)\n",
      "Requirement already satisfied: statsmodels>=0.8.0 in /anaconda3/lib/python3.6/site-packages (from tsfresh) (0.8.0)\n",
      "Requirement already satisfied: scikit-learn>=0.17.1 in /anaconda3/lib/python3.6/site-packages (from tsfresh) (0.19.1)\n",
      "Requirement already satisfied: tqdm>=4.10.0 in /anaconda3/lib/python3.6/site-packages (from tsfresh) (4.19.2)\n",
      "Requirement already satisfied: scipy>=0.17.0 in /anaconda3/lib/python3.6/site-packages (from tsfresh) (1.0.0)\n",
      "Requirement already satisfied: future>=0.16.0 in /anaconda3/lib/python3.6/site-packages (from tsfresh) (0.16.0)\n",
      "Requirement already satisfied: six>=1.10.0 in /anaconda3/lib/python3.6/site-packages (from tsfresh) (1.11.0)\n",
      "Requirement already satisfied: requests>=2.9.1 in /anaconda3/lib/python3.6/site-packages (from tsfresh) (2.18.4)\n",
      "Requirement already satisfied: click>=6.6 in /anaconda3/lib/python3.6/site-packages (from distributed>=1.18.3->tsfresh) (6.7)\n",
      "Requirement already satisfied: cloudpickle>=0.2.2 in /anaconda3/lib/python3.6/site-packages (from distributed>=1.18.3->tsfresh) (0.5.2)\n",
      "Requirement already satisfied: msgpack-python in /anaconda3/lib/python3.6/site-packages (from distributed>=1.18.3->tsfresh) (0.5.1)\n",
      "Requirement already satisfied: psutil in /anaconda3/lib/python3.6/site-packages (from distributed>=1.18.3->tsfresh) (5.4.3)\n",
      "Requirement already satisfied: sortedcontainers in /anaconda3/lib/python3.6/site-packages (from distributed>=1.18.3->tsfresh) (1.5.9)\n",
      "Requirement already satisfied: tblib in /anaconda3/lib/python3.6/site-packages (from distributed>=1.18.3->tsfresh) (1.3.2)\n",
      "Requirement already satisfied: toolz>=0.7.4 in /anaconda3/lib/python3.6/site-packages (from distributed>=1.18.3->tsfresh) (0.9.0)\n",
      "Requirement already satisfied: tornado>=4.5.1 in /anaconda3/lib/python3.6/site-packages (from distributed>=1.18.3->tsfresh) (4.5.3)\n",
      "Requirement already satisfied: zict>=0.1.3 in /anaconda3/lib/python3.6/site-packages (from distributed>=1.18.3->tsfresh) (0.1.3)\n",
      "Requirement already satisfied: pytz>=2011k in /anaconda3/lib/python3.6/site-packages (from pandas>=0.20.3->tsfresh) (2017.3)\n",
      "Requirement already satisfied: python-dateutil>=2 in /anaconda3/lib/python3.6/site-packages (from pandas>=0.20.3->tsfresh) (2.6.1)\n",
      "Requirement already satisfied: chardet<3.1.0,>=3.0.2 in /anaconda3/lib/python3.6/site-packages (from requests>=2.9.1->tsfresh) (3.0.4)\n",
      "Requirement already satisfied: idna<2.7,>=2.5 in /anaconda3/lib/python3.6/site-packages (from requests>=2.9.1->tsfresh) (2.6)\n",
      "Requirement already satisfied: urllib3<1.23,>=1.21.1 in /anaconda3/lib/python3.6/site-packages (from requests>=2.9.1->tsfresh) (1.22)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /anaconda3/lib/python3.6/site-packages (from requests>=2.9.1->tsfresh) (2018.8.24)\n",
      "Requirement already satisfied: heapdict in /anaconda3/lib/python3.6/site-packages (from zict>=0.1.3->distributed>=1.18.3->tsfresh) (1.0.0)\n",
      "Installing collected packages: tsfresh\n",
      "Successfully installed tsfresh-0.11.1\n"
     ]
    }
   ],
   "source": [
    "!pip install tsfresh"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/anaconda3/lib/python3.6/site-packages/lightgbm/__init__.py:46: UserWarning: Starting from version 2.2.1, the library file in distribution wheels for macOS is built by the Apple Clang (Xcode_8.3.1) compiler.\n",
      "This means that in case of installing LightGBM from PyPI via the ``pip install lightgbm`` command, you don't need to install the gcc compiler anymore.\n",
      "Instead of that, you need to install the OpenMP library, which is required for running LightGBM on the system with the Apple Clang compiler.\n",
      "You can install the OpenMP library by the following command: ``brew install libomp``.\n",
      "  \"You can install the OpenMP library by the following command: ``brew install libomp``.\", UserWarning)\n",
      "/anaconda3/lib/python3.6/site-packages/statsmodels/compat/pandas.py:56: FutureWarning: The pandas.core.datetools module is deprecated and will be removed in a future version. Please use the pandas.tseries module instead.\n",
      "  from pandas.core import datetools\n"
     ]
    }
   ],
   "source": [
    "import numpy as np # linear algebra\n",
    "import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n",
    "from sklearn.metrics import log_loss\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "\n",
    "import os\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns \n",
    "import lightgbm as lgb\n",
    "from catboost import Pool, CatBoostClassifier\n",
    "import itertools\n",
    "import pickle, gzip\n",
    "import glob\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from tsfresh.feature_extraction import extract_features\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_folder = \"/Users/youssefberrada/Downloads/all/\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Acquisition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "train = pd.read_csv(data_folder+'training_set.csv')\n",
    "meta_train = pd.read_csv(data_folder+'training_set_metadata.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>object_id</th>\n",
       "      <th>mjd</th>\n",
       "      <th>passband</th>\n",
       "      <th>flux</th>\n",
       "      <th>flux_err</th>\n",
       "      <th>detected</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>615</td>\n",
       "      <td>59750.4229</td>\n",
       "      <td>2</td>\n",
       "      <td>-544.810303</td>\n",
       "      <td>3.622952</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>615</td>\n",
       "      <td>59750.4306</td>\n",
       "      <td>1</td>\n",
       "      <td>-816.434326</td>\n",
       "      <td>5.553370</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>615</td>\n",
       "      <td>59750.4383</td>\n",
       "      <td>3</td>\n",
       "      <td>-471.385529</td>\n",
       "      <td>3.801213</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>615</td>\n",
       "      <td>59750.4450</td>\n",
       "      <td>4</td>\n",
       "      <td>-388.984985</td>\n",
       "      <td>11.395031</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>615</td>\n",
       "      <td>59752.4070</td>\n",
       "      <td>2</td>\n",
       "      <td>-681.858887</td>\n",
       "      <td>4.041204</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   object_id         mjd  passband        flux   flux_err  detected\n",
       "0        615  59750.4229         2 -544.810303   3.622952         1\n",
       "1        615  59750.4306         1 -816.434326   5.553370         1\n",
       "2        615  59750.4383         3 -471.385529   3.801213         1\n",
       "3        615  59750.4450         4 -388.984985  11.395031         1\n",
       "4        615  59752.4070         2 -681.858887   4.041204         1"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Pre-Processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "fcp = {'fft_coefficient': [{'coeff': 0, 'attr': 'abs'},{'coeff': 1, 'attr': 'abs'}],'kurtosis' : None, 'skewness' : None}\n",
    "\n",
    "def featurize(df):\n",
    "    df['flux_ratio_sq'] = np.power(df['flux'] / df['flux_err'], 2.0)\n",
    "    df['flux_by_flux_ratio_sq'] = df['flux'] * df['flux_ratio_sq']\n",
    "    # train[detected==1, mjd_diff:=max(mjd)-min(mjd), by=object_id]\n",
    "\n",
    "\n",
    "    aggs = {\n",
    "        'flux': ['min', 'max', 'mean', 'median', 'std','skew'],\n",
    "        'flux_err': ['min', 'max', 'mean', 'median', 'std','skew'],\n",
    "        'detected': ['mean'],\n",
    "        'flux_ratio_sq':['sum','skew'],\n",
    "        'flux_by_flux_ratio_sq':['sum','skew'],\n",
    "    }\n",
    "\n",
    "    agg_df = df.groupby('object_id').agg(aggs)\n",
    "    new_columns = [\n",
    "        k + '_' + agg for k in aggs.keys() for agg in aggs[k]\n",
    "    ]\n",
    "    agg_df.columns = new_columns\n",
    "    agg_df['flux_diff'] = agg_df['flux_max'] - agg_df['flux_min']\n",
    "    agg_df['flux_dif2'] = (agg_df['flux_max'] - agg_df['flux_min']) / agg_df['flux_mean']\n",
    "    agg_df['flux_w_mean'] = agg_df['flux_by_flux_ratio_sq_sum'] / agg_df['flux_ratio_sq_sum']\n",
    "    agg_df['flux_dif3'] = (agg_df['flux_max'] - agg_df['flux_min']) / agg_df['flux_w_mean']\n",
    "    # Add more features with \n",
    "    agg_df_ts = extract_features(df, column_id='object_id', column_sort='mjd', column_kind='passband', column_value = 'flux', default_fc_parameters = fcp, n_jobs=4)\n",
    "    # Add smart feature that is suggested here https://www.kaggle.com/c/PLAsTiCC-2018/discussion/69696#410538\n",
    "    # dt[detected==1, mjd_diff:=max(mjd)-min(mjd), by=object_id]\n",
    "    df_det = df[df['detected']==1].copy()\n",
    "\n",
    "    agg_df_mjd = extract_features(df_det, column_id='object_id', column_value = 'mjd', default_fc_parameters = {'maximum':None, 'minimum':None}, n_jobs=4)\n",
    "    agg_df_mjd['mjd_diff_det'] = agg_df_mjd['mjd__maximum'] - agg_df_mjd['mjd__minimum']\n",
    "    del agg_df_mjd['mjd__maximum'], agg_df_mjd['mjd__minimum']\n",
    "    agg_df_ts = pd.merge(agg_df_ts, agg_df_mjd,left_index=True, right_index=True)\n",
    "    # tsfresh returns a dataframe with an index name='id'\n",
    "    #agg_df_ts.index.rename('object_id',inplace=True)\n",
    "    agg_df = pd.merge(agg_df, agg_df_ts,left_index=True, right_index=True)\n",
    "    return agg_df\n",
    "    #return agg_df_mjd,agg_df_ts\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Feature Extraction: 100%|██████████| 20/20 [00:10<00:00,  1.94it/s]\n",
      "Feature Extraction: 100%|██████████| 20/20 [00:01<00:00, 19.35it/s]\n"
     ]
    }
   ],
   "source": [
    "agg_train= featurize(train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>flux_min</th>\n",
       "      <th>flux_max</th>\n",
       "      <th>flux_mean</th>\n",
       "      <th>flux_median</th>\n",
       "      <th>flux_std</th>\n",
       "      <th>flux_skew</th>\n",
       "      <th>flux_err_min</th>\n",
       "      <th>flux_err_max</th>\n",
       "      <th>flux_err_mean</th>\n",
       "      <th>flux_err_median</th>\n",
       "      <th>...</th>\n",
       "      <th>3__skewness</th>\n",
       "      <th>4__fft_coefficient__coeff_0__attr_\"abs\"</th>\n",
       "      <th>4__fft_coefficient__coeff_1__attr_\"abs\"</th>\n",
       "      <th>4__kurtosis</th>\n",
       "      <th>4__skewness</th>\n",
       "      <th>5__fft_coefficient__coeff_0__attr_\"abs\"</th>\n",
       "      <th>5__fft_coefficient__coeff_1__attr_\"abs\"</th>\n",
       "      <th>5__kurtosis</th>\n",
       "      <th>5__skewness</th>\n",
       "      <th>mjd_diff_det</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>object_id</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>615</th>\n",
       "      <td>-1100.440063</td>\n",
       "      <td>660.626343</td>\n",
       "      <td>-123.096998</td>\n",
       "      <td>-89.477524</td>\n",
       "      <td>394.109851</td>\n",
       "      <td>-0.349540</td>\n",
       "      <td>2.130510</td>\n",
       "      <td>12.845472</td>\n",
       "      <td>4.482743</td>\n",
       "      <td>3.835269</td>\n",
       "      <td>...</td>\n",
       "      <td>0.293128</td>\n",
       "      <td>3245.366349</td>\n",
       "      <td>2741.539785</td>\n",
       "      <td>-1.548319</td>\n",
       "      <td>0.200096</td>\n",
       "      <td>2704.641265</td>\n",
       "      <td>2893.344217</td>\n",
       "      <td>-1.592820</td>\n",
       "      <td>0.125268</td>\n",
       "      <td>873.7903</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>713</th>\n",
       "      <td>-14.735178</td>\n",
       "      <td>14.770886</td>\n",
       "      <td>-1.423351</td>\n",
       "      <td>-0.873033</td>\n",
       "      <td>6.471144</td>\n",
       "      <td>0.014989</td>\n",
       "      <td>0.639458</td>\n",
       "      <td>9.115748</td>\n",
       "      <td>2.359620</td>\n",
       "      <td>1.998217</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.167176</td>\n",
       "      <td>50.414646</td>\n",
       "      <td>203.892482</td>\n",
       "      <td>-1.190245</td>\n",
       "      <td>-0.064134</td>\n",
       "      <td>100.473776</td>\n",
       "      <td>143.963093</td>\n",
       "      <td>-0.797047</td>\n",
       "      <td>0.218182</td>\n",
       "      <td>846.8017</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>730</th>\n",
       "      <td>-19.159811</td>\n",
       "      <td>47.310059</td>\n",
       "      <td>2.267434</td>\n",
       "      <td>0.409172</td>\n",
       "      <td>8.022239</td>\n",
       "      <td>3.177854</td>\n",
       "      <td>0.695106</td>\n",
       "      <td>11.281384</td>\n",
       "      <td>2.471061</td>\n",
       "      <td>1.990851</td>\n",
       "      <td>...</td>\n",
       "      <td>2.662075</td>\n",
       "      <td>219.745132</td>\n",
       "      <td>202.532898</td>\n",
       "      <td>6.081065</td>\n",
       "      <td>2.537802</td>\n",
       "      <td>231.509177</td>\n",
       "      <td>199.286370</td>\n",
       "      <td>3.583130</td>\n",
       "      <td>1.680352</td>\n",
       "      <td>78.7737</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>745</th>\n",
       "      <td>-15.494463</td>\n",
       "      <td>220.795212</td>\n",
       "      <td>8.909206</td>\n",
       "      <td>1.035895</td>\n",
       "      <td>27.558208</td>\n",
       "      <td>4.979826</td>\n",
       "      <td>0.567170</td>\n",
       "      <td>55.892746</td>\n",
       "      <td>2.555576</td>\n",
       "      <td>1.819875</td>\n",
       "      <td>...</td>\n",
       "      <td>3.751603</td>\n",
       "      <td>735.528417</td>\n",
       "      <td>680.055280</td>\n",
       "      <td>13.747434</td>\n",
       "      <td>3.476420</td>\n",
       "      <td>591.037583</td>\n",
       "      <td>523.503586</td>\n",
       "      <td>12.134629</td>\n",
       "      <td>3.170857</td>\n",
       "      <td>123.6872</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1124</th>\n",
       "      <td>-16.543753</td>\n",
       "      <td>143.600189</td>\n",
       "      <td>7.145702</td>\n",
       "      <td>1.141288</td>\n",
       "      <td>20.051722</td>\n",
       "      <td>4.406298</td>\n",
       "      <td>0.695277</td>\n",
       "      <td>11.383690</td>\n",
       "      <td>2.753004</td>\n",
       "      <td>2.214854</td>\n",
       "      <td>...</td>\n",
       "      <td>3.603208</td>\n",
       "      <td>574.553907</td>\n",
       "      <td>524.107264</td>\n",
       "      <td>16.377058</td>\n",
       "      <td>3.904008</td>\n",
       "      <td>393.114268</td>\n",
       "      <td>357.907185</td>\n",
       "      <td>14.434470</td>\n",
       "      <td>3.657305</td>\n",
       "      <td>133.9113</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 46 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "              flux_min    flux_max   flux_mean  flux_median    flux_std  \\\n",
       "object_id                                                                 \n",
       "615       -1100.440063  660.626343 -123.096998   -89.477524  394.109851   \n",
       "713         -14.735178   14.770886   -1.423351    -0.873033    6.471144   \n",
       "730         -19.159811   47.310059    2.267434     0.409172    8.022239   \n",
       "745         -15.494463  220.795212    8.909206     1.035895   27.558208   \n",
       "1124        -16.543753  143.600189    7.145702     1.141288   20.051722   \n",
       "\n",
       "           flux_skew  flux_err_min  flux_err_max  flux_err_mean  \\\n",
       "object_id                                                         \n",
       "615        -0.349540      2.130510     12.845472       4.482743   \n",
       "713         0.014989      0.639458      9.115748       2.359620   \n",
       "730         3.177854      0.695106     11.281384       2.471061   \n",
       "745         4.979826      0.567170     55.892746       2.555576   \n",
       "1124        4.406298      0.695277     11.383690       2.753004   \n",
       "\n",
       "           flux_err_median      ...       3__skewness  \\\n",
       "object_id                       ...                     \n",
       "615               3.835269      ...          0.293128   \n",
       "713               1.998217      ...         -0.167176   \n",
       "730               1.990851      ...          2.662075   \n",
       "745               1.819875      ...          3.751603   \n",
       "1124              2.214854      ...          3.603208   \n",
       "\n",
       "           4__fft_coefficient__coeff_0__attr_\"abs\"  \\\n",
       "object_id                                            \n",
       "615                                    3245.366349   \n",
       "713                                      50.414646   \n",
       "730                                     219.745132   \n",
       "745                                     735.528417   \n",
       "1124                                    574.553907   \n",
       "\n",
       "           4__fft_coefficient__coeff_1__attr_\"abs\"  4__kurtosis  4__skewness  \\\n",
       "object_id                                                                      \n",
       "615                                    2741.539785    -1.548319     0.200096   \n",
       "713                                     203.892482    -1.190245    -0.064134   \n",
       "730                                     202.532898     6.081065     2.537802   \n",
       "745                                     680.055280    13.747434     3.476420   \n",
       "1124                                    524.107264    16.377058     3.904008   \n",
       "\n",
       "           5__fft_coefficient__coeff_0__attr_\"abs\"  \\\n",
       "object_id                                            \n",
       "615                                    2704.641265   \n",
       "713                                     100.473776   \n",
       "730                                     231.509177   \n",
       "745                                     591.037583   \n",
       "1124                                    393.114268   \n",
       "\n",
       "           5__fft_coefficient__coeff_1__attr_\"abs\"  5__kurtosis  5__skewness  \\\n",
       "object_id                                                                      \n",
       "615                                    2893.344217    -1.592820     0.125268   \n",
       "713                                     143.963093    -0.797047     0.218182   \n",
       "730                                     199.286370     3.583130     1.680352   \n",
       "745                                     523.503586    12.134629     3.170857   \n",
       "1124                                    357.907185    14.434470     3.657305   \n",
       "\n",
       "           mjd_diff_det  \n",
       "object_id                \n",
       "615            873.7903  \n",
       "713            846.8017  \n",
       "730             78.7737  \n",
       "745            123.6872  \n",
       "1124           133.9113  \n",
       "\n",
       "[5 rows x 46 columns]"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "agg_train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unique classes :  [6, 15, 16, 42, 52, 53, 62, 64, 65, 67, 88, 90, 92, 95]\n"
     ]
    }
   ],
   "source": [
    "full_train = agg_train.reset_index().merge(\n",
    "    right=meta_train,\n",
    "    how='outer',\n",
    "    on='object_id'\n",
    ")\n",
    "\n",
    "if 'target' in full_train:\n",
    "    y = full_train['target']\n",
    "    del full_train['target']\n",
    "classes = sorted(y.unique())\n",
    "\n",
    "class_weight = {\n",
    "    c: 1 for c in classes\n",
    "}\n",
    "for c in [64, 15]:\n",
    "    class_weight[c] = 2\n",
    "\n",
    "print('Unique classes : ', classes)\n",
    "\n",
    "if 'object_id' in full_train:\n",
    "    oof_df = full_train[['object_id']]\n",
    "    del full_train['object_id'], full_train['distmod'], full_train['hostgal_specz']\n",
    "    del full_train['ra'], full_train['decl'], full_train['gal_l'],full_train['gal_b'],full_train['ddf']\n",
    "    \n",
    "train_mean = full_train.mean(axis=0)\n",
    "full_train.fillna(0, inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Light GBM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training until validation scores don't improve for 50 rounds.\n",
      "[100]\ttraining's multi_logloss: 0.823306\ttraining's wloss: 0.814514\tvalid_1's multi_logloss: 1.1738\tvalid_1's wloss: 0.975114\n",
      "[200]\ttraining's multi_logloss: 0.56155\ttraining's wloss: 0.549672\tvalid_1's multi_logloss: 0.948133\tvalid_1's wloss: 0.780383\n",
      "[300]\ttraining's multi_logloss: 0.444386\ttraining's wloss: 0.432032\tvalid_1's multi_logloss: 0.860011\tvalid_1's wloss: 0.733548\n",
      "[400]\ttraining's multi_logloss: 0.373985\ttraining's wloss: 0.361467\tvalid_1's multi_logloss: 0.812887\tvalid_1's wloss: 0.715265\n",
      "[500]\ttraining's multi_logloss: 0.32359\ttraining's wloss: 0.311557\tvalid_1's multi_logloss: 0.780668\tvalid_1's wloss: 0.706554\n",
      "Early stopping, best iteration is:\n",
      "[531]\ttraining's multi_logloss: 0.310625\ttraining's wloss: 0.298837\tvalid_1's multi_logloss: 0.771908\tvalid_1's wloss: 0.704557\n",
      "0.7045571282851983\n",
      "Training until validation scores don't improve for 50 rounds.\n",
      "[100]\ttraining's multi_logloss: 0.821123\ttraining's wloss: 0.813059\tvalid_1's multi_logloss: 1.16207\tvalid_1's wloss: 0.976192\n",
      "[200]\ttraining's multi_logloss: 0.559908\ttraining's wloss: 0.548312\tvalid_1's multi_logloss: 0.940445\tvalid_1's wloss: 0.799609\n",
      "[300]\ttraining's multi_logloss: 0.445433\ttraining's wloss: 0.433705\tvalid_1's multi_logloss: 0.859577\tvalid_1's wloss: 0.778837\n",
      "Early stopping, best iteration is:\n",
      "[278]\ttraining's multi_logloss: 0.465111\ttraining's wloss: 0.453382\tvalid_1's multi_logloss: 0.87271\tvalid_1's wloss: 0.777936\n",
      "0.7779356564031681\n",
      "Training until validation scores don't improve for 50 rounds.\n",
      "[100]\ttraining's multi_logloss: 0.818528\ttraining's wloss: 0.810182\tvalid_1's multi_logloss: 1.18263\tvalid_1's wloss: 0.950512\n",
      "[200]\ttraining's multi_logloss: 0.559446\ttraining's wloss: 0.547523\tvalid_1's multi_logloss: 0.952384\tvalid_1's wloss: 0.741728\n",
      "[300]\ttraining's multi_logloss: 0.443605\ttraining's wloss: 0.431263\tvalid_1's multi_logloss: 0.863623\tvalid_1's wloss: 0.686614\n",
      "[400]\ttraining's multi_logloss: 0.373883\ttraining's wloss: 0.36158\tvalid_1's multi_logloss: 0.813976\tvalid_1's wloss: 0.664144\n",
      "[500]\ttraining's multi_logloss: 0.324333\ttraining's wloss: 0.312459\tvalid_1's multi_logloss: 0.780962\tvalid_1's wloss: 0.651435\n",
      "[600]\ttraining's multi_logloss: 0.285457\ttraining's wloss: 0.274276\tvalid_1's multi_logloss: 0.756767\tvalid_1's wloss: 0.647222\n",
      "Early stopping, best iteration is:\n",
      "[645]\ttraining's multi_logloss: 0.270179\ttraining's wloss: 0.259332\tvalid_1's multi_logloss: 0.746781\tvalid_1's wloss: 0.645345\n",
      "0.645345039439049\n",
      "Training until validation scores don't improve for 50 rounds.\n",
      "[100]\ttraining's multi_logloss: 0.82737\ttraining's wloss: 0.816934\tvalid_1's multi_logloss: 1.17139\tvalid_1's wloss: 0.957642\n",
      "[200]\ttraining's multi_logloss: 0.566601\ttraining's wloss: 0.554313\tvalid_1's multi_logloss: 0.935065\tvalid_1's wloss: 0.746764\n",
      "[300]\ttraining's multi_logloss: 0.448454\ttraining's wloss: 0.435594\tvalid_1's multi_logloss: 0.83943\tvalid_1's wloss: 0.692299\n",
      "[400]\ttraining's multi_logloss: 0.37606\ttraining's wloss: 0.363268\tvalid_1's multi_logloss: 0.786437\tvalid_1's wloss: 0.674016\n",
      "[500]\ttraining's multi_logloss: 0.323602\ttraining's wloss: 0.311478\tvalid_1's multi_logloss: 0.750078\tvalid_1's wloss: 0.66917\n",
      "Early stopping, best iteration is:\n",
      "[483]\ttraining's multi_logloss: 0.331761\ttraining's wloss: 0.319488\tvalid_1's multi_logloss: 0.755284\tvalid_1's wloss: 0.66768\n",
      "0.667680168436726\n",
      "Training until validation scores don't improve for 50 rounds.\n",
      "[100]\ttraining's multi_logloss: 0.818511\ttraining's wloss: 0.808121\tvalid_1's multi_logloss: 1.17926\tvalid_1's wloss: 0.989307\n",
      "[200]\ttraining's multi_logloss: 0.557187\ttraining's wloss: 0.54475\tvalid_1's multi_logloss: 0.942358\tvalid_1's wloss: 0.78889\n",
      "[300]\ttraining's multi_logloss: 0.441082\ttraining's wloss: 0.428842\tvalid_1's multi_logloss: 0.84469\tvalid_1's wloss: 0.73924\n",
      "[400]\ttraining's multi_logloss: 0.370704\ttraining's wloss: 0.358756\tvalid_1's multi_logloss: 0.791909\tvalid_1's wloss: 0.720516\n",
      "[500]\ttraining's multi_logloss: 0.321137\ttraining's wloss: 0.309804\tvalid_1's multi_logloss: 0.757235\tvalid_1's wloss: 0.713863\n",
      "Early stopping, best iteration is:\n",
      "[492]\ttraining's multi_logloss: 0.324475\ttraining's wloss: 0.313046\tvalid_1's multi_logloss: 0.759681\tvalid_1's wloss: 0.713699\n",
      "0.7136990516523407\n"
     ]
    }
   ],
   "source": [
    "folds = StratifiedKFold(n_splits=5, shuffle=True, random_state=1)\n",
    "clfs = []\n",
    "importances = pd.DataFrame()\n",
    "lgb_params = {\n",
    "    'boosting_type': 'gbdt',\n",
    "    'objective': 'multiclass',\n",
    "    'num_class': 14,\n",
    "    'metric': 'multi_logloss',\n",
    "    'learning_rate': 0.03,\n",
    "    'subsample': .9,\n",
    "    'colsample_bytree': 0.5,\n",
    "    'reg_alpha': .01,\n",
    "    'reg_lambda': .01,\n",
    "    'min_split_gain': 0.01,\n",
    "    'min_child_weight': 10,\n",
    "    'n_estimators': 1000,\n",
    "    'silent': -1,\n",
    "    'verbose': -1,\n",
    "    'max_depth': 3\n",
    "}\n",
    "\n",
    "# Compute weights\n",
    "w = y.value_counts()\n",
    "weights = {i : np.sum(w) / w[i] for i in w.index}\n",
    "\n",
    "oof_preds = np.zeros((len(full_train), np.unique(y).shape[0]))\n",
    "for fold_, (trn_, val_) in enumerate(folds.split(y, y)):\n",
    "    trn_x, trn_y = full_train.iloc[trn_], y.iloc[trn_]\n",
    "    val_x, val_y = full_train.iloc[val_], y.iloc[val_]\n",
    "\n",
    "    clf = lgb.LGBMClassifier(**lgb_params)\n",
    "    clf.fit(\n",
    "        trn_x, trn_y,\n",
    "        eval_set=[(trn_x, trn_y), (val_x, val_y)],\n",
    "        eval_metric=lgb_multi_weighted_logloss,\n",
    "        verbose=100,\n",
    "        early_stopping_rounds=50,\n",
    "        sample_weight=trn_y.map(weights)\n",
    "    )\n",
    "    oof_preds[val_, :] = clf.predict_proba(val_x, num_iteration=clf.best_iteration_)\n",
    "    print(multi_weighted_logloss(val_y, oof_preds[val_, :]))\n",
    "\n",
    "    imp_df = pd.DataFrame()\n",
    "    imp_df['feature'] = full_train.columns\n",
    "    imp_df['gain'] = clf.feature_importances_\n",
    "    imp_df['fold'] = fold_ + 1\n",
    "    importances = pd.concat([importances, imp_df], axis=0)\n",
    "\n",
    "    clfs.append(clf)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_chunk(df_, clfs_, meta_, features, train_mean):\n",
    "    # Group by object id    \n",
    "    agg_ = featurize(df_)\n",
    "    # Merge with meta data\n",
    "    full_test = agg_.reset_index().merge(\n",
    "        right=meta_,\n",
    "        how='left',\n",
    "        on='object_id'\n",
    "    )\n",
    "\n",
    "    full_test = full_test.fillna(0)\n",
    "    # Make predictions\n",
    "    preds_ = None\n",
    "    for clf in clfs_:\n",
    "        if preds_ is None:\n",
    "            preds_ = clf.predict_proba(full_test[features]) / len(clfs_)\n",
    "        else:\n",
    "            preds_ += clf.predict_proba(full_test[features]) / len(clfs_)\n",
    "\n",
    "    # Compute preds_99 as the proba of class not being any of the others\n",
    "    # preds_99 = 0.1 gives 1.769\n",
    "    preds_99 = np.ones(preds_.shape[0])\n",
    "    for i in range(preds_.shape[1]):\n",
    "        preds_99 *= (1 - preds_[:, i])\n",
    "\n",
    "    # Create DataFrame from predictions\n",
    "    preds_df_ = pd.DataFrame(preds_, columns=['class_' + str(s) for s in clfs_[0].classes_])\n",
    "    preds_df_['object_id'] = full_test['object_id']\n",
    "    preds_df_['class_99'] = 0.14 * preds_99 / np.mean(preds_99) \n",
    "    return preds_df_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "meta_test = pd.read_csv(data_folder+'test_set_metadata.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# meta_test.set_index('object_id',inplace=True)\n",
    "import time\n",
    "\n",
    "start = time.time()\n",
    "chunks = 5000000\n",
    "remain_df = None\n",
    "\n",
    "for i_c, df in enumerate(pd.read_csv('../input/test_set.csv', chunksize=chunks, iterator=True)):\n",
    "    unique_ids = np.unique(df['object_id'])\n",
    "    new_remain_df = df.loc[df['object_id'] == unique_ids[-1]].copy()\n",
    "    if remain_df is None:\n",
    "        df = df.loc[df['object_id'].isin(unique_ids[:-1])]\n",
    "    else:\n",
    "        df = pd.concat([remain_df, df.loc[df['object_id'].isin(unique_ids[:-1])]], axis=0)\n",
    "    # Create remaining samples df\n",
    "    remain_df = new_remain_df\n",
    "    preds_df = predict_chunk(df_=df,\n",
    "                             clfs_=clfs,\n",
    "                             meta_=meta_test,\n",
    "                             features=full_train.columns,\n",
    "                             train_mean=train_mean)\n",
    "\n",
    "    if i_c == 0:\n",
    "        preds_df.to_csv('predictions.csv', header=True, mode='a', index=False)\n",
    "    else:\n",
    "        preds_df.to_csv('predictions.csv', header=False, mode='a', index=False)\n",
    "\n",
    "    del preds_df\n",
    "    gc.collect()\n",
    "    \n",
    "    print('%15d done in %5.1f minutes' % (chunks * (i_c + 1), (time.time() - start) / 60), flush=True)\n",
    "\n",
    "# Compute last object in remain_df\n",
    "preds_df = predict_chunk(df_=remain_df,\n",
    "                         clfs_=clfs,\n",
    "                         meta_=meta_test,\n",
    "                         features=full_train.columns,\n",
    "                         train_mean=train_mean)\n",
    "\n",
    "preds_df.to_csv('predictions.csv', header=False, mode='a', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Evaluation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Loss Function "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "def multi_weighted_logloss(y_true, y_preds):\n",
    "    \"\"\"\n",
    "    this function implements a multi logloss \n",
    "    \"\"\"\n",
    "    classes = [6, 15, 16, 42, 52, 53, 62, 64, 65, 67, 88, 90, 92, 95]\n",
    "    class_weight = {6: 1, 15: 2, 16: 1, 42: 1, 52: 1, 53: 1, 62: 1, 64: 2, 65: 1, 67: 1, 88: 1, 90: 1, 92: 1, 95: 1}\n",
    "    if len(np.unique(y_true)) > 14:\n",
    "        classes.append(99)\n",
    "        class_weight[99] = 2\n",
    "    y_p = y_preds\n",
    "    # Trasform y_true in dummies\n",
    "    y_ohe = pd.get_dummies(y_true)\n",
    "    # Normalize rows and limit y_preds to 1e-15, 1-1e-15\n",
    "    y_p = np.clip(a=y_p, a_min=1e-15, a_max=1 - 1e-15)\n",
    "    # Transform to log\n",
    "    y_p_log = np.log(y_p)\n",
    "    # Get the log for ones, .values is used to drop the index of DataFrames\n",
    "    # Exclude class 99 for now, since there is no class99 in the training set\n",
    "    # we gave a special process for that class\n",
    "    y_log_ones = np.sum(y_ohe.values * y_p_log, axis=0)\n",
    "    # Get the number of positives for each class\n",
    "    nb_pos = y_ohe.sum(axis=0).values.astype(float)\n",
    "    # Weight average and divide by the number of positives\n",
    "    class_arr = np.array([class_weight[k] for k in sorted(class_weight.keys())])\n",
    "    y_w = y_log_ones * class_arr / nb_pos\n",
    "\n",
    "    loss = - np.sum(y_w) / np.sum(class_arr)\n",
    "    return loss\n",
    "def lgb_multi_weighted_logloss(y_true, y_preds):\n",
    "    \"\"\"\n",
    "    @author olivier https://www.kaggle.com/ogrellier\n",
    "    multi logloss for PLAsTiCC challenge\n",
    "    \"\"\"\n",
    "    # class_weights taken from Giba's topic : https://www.kaggle.com/titericz\n",
    "    # https://www.kaggle.com/c/PLAsTiCC-2018/discussion/67194\n",
    "    # with Kyle Boone's post https://www.kaggle.com/kyleboone\n",
    "    classes = [6, 15, 16, 42, 52, 53, 62, 64, 65, 67, 88, 90, 92, 95]\n",
    "    class_weight = {6: 1, 15: 2, 16: 1, 42: 1, 52: 1, 53: 1, 62: 1, 64: 2, 65: 1, 67: 1, 88: 1, 90: 1, 92: 1, 95: 1}\n",
    "    if len(np.unique(y_true)) > 14:\n",
    "        classes.append(99)\n",
    "        class_weight[99] = 2\n",
    "    y_p = y_preds.reshape(y_true.shape[0], len(classes), order='F')\n",
    "\n",
    "    # Trasform y_true in dummies\n",
    "    y_ohe = pd.get_dummies(y_true)\n",
    "    # Normalize rows and limit y_preds to 1e-15, 1-1e-15\n",
    "    y_p = np.clip(a=y_p, a_min=1e-15, a_max=1 - 1e-15)\n",
    "    # Transform to log\n",
    "    y_p_log = np.log(y_p)\n",
    "    # Get the log for ones, .values is used to drop the index of DataFrames\n",
    "    # Exclude class 99 for now, since there is no class99 in the training set\n",
    "    # we gave a special process for that class\n",
    "    y_log_ones = np.sum(y_ohe.values * y_p_log, axis=0)\n",
    "    # Get the number of positives for each class\n",
    "    nb_pos = y_ohe.sum(axis=0).values.astype(float)\n",
    "    # Weight average and divide by the number of positives\n",
    "    class_arr = np.array([class_weight[k] for k in sorted(class_weight.keys())])\n",
    "    y_w = y_log_ones * class_arr / nb_pos\n",
    "\n",
    "    loss = - np.sum(y_w) / np.sum(class_arr)\n",
    "    return 'wloss', loss, False\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MULTI WEIGHTED LOG LOSS : 34.53878 \n"
     ]
    }
   ],
   "source": [
    "print('MULTI WEIGHTED LOG LOSS : %.5f ' % multi_weighted_logloss(y_true=y, y_preds=oof_preds))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Performance Measure\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Precision"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import precision_score\n",
    "precision_sc = precision_score(y_map, np.argmax(oof_preds,axis=-1), average='macro')\n",
    "precision_sc"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Recall"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import recall_score\n",
    "recall_sc = recall_score(y_map, np.argmax(oof_preds,axis=-1), average='macro')\n",
    "recall_sc"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### F-1 Score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import f1_score\n",
    "f1_sc = f1_score(y_map, np.argmax(oof_preds,axis=-1), average='macro')\n",
    "f1_sc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Presentation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Confusion Matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import confusion_matrix\n",
    "cnf_matrix = confusion_matrix(y_map, np.argmax(oof_preds,axis=-1))\n",
    "np.set_printoptions(precision=2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Plot the Confusion Matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_confusion_matrix(cm, classes,\n",
    "                          normalize=False,\n",
    "                          title='Confusion matrix',\n",
    "                          cmap=plt.cm.Blues):\n",
    "    \"\"\"\n",
    "    This function prints and plots the confusion matrix.\n",
    "    Normalization can be applied by setting `normalize=True`.\n",
    "    \"\"\"\n",
    "    if normalize:\n",
    "        cm = cm.astype('float') / cm.sum(axis=1)[:, np.newaxis]\n",
    "        print(\"Normalized confusion matrix\")\n",
    "    else:\n",
    "        print('Confusion matrix, without normalization')\n",
    "\n",
    "    plt.imshow(cm, interpolation='nearest', cmap=cmap)\n",
    "    plt.title(title)\n",
    "    plt.colorbar()\n",
    "    tick_marks = np.arange(len(classes))\n",
    "    plt.xticks(tick_marks, classes, rotation=45)\n",
    "    plt.yticks(tick_marks, classes)\n",
    "\n",
    "    fmt = '.2f' if normalize else 'd'\n",
    "    thresh = cm.max() / 2.\n",
    "    for i, j in itertools.product(range(cm.shape[0]), range(cm.shape[1])):\n",
    "        plt.text(j, i, format(cm[i, j], fmt),\n",
    "                 horizontalalignment=\"center\",\n",
    "                 color=\"white\" if cm[i, j] > thresh else \"black\")\n",
    "\n",
    "    plt.ylabel('True label')\n",
    "    plt.xlabel('Predicted label')\n",
    "    plt.tight_layout()\n",
    "    \n",
    "unique_y = np.unique(y)\n",
    "class_map = dict()\n",
    "for i,val in enumerate(unique_y):\n",
    "    class_map[val] = i\n",
    "        \n",
    "y_map = np.zeros((y.shape[0],))\n",
    "y_map = np.array([class_map[val] for val in y])\n",
    "\n",
    "# Compute confusion matrix\n",
    "from sklearn.metrics import confusion_matrix\n",
    "cnf_matrix = confusion_matrix(y_map, np.argmax(oof_preds,axis=-1))\n",
    "np.set_printoptions(precision=2)\n",
    "\n",
    "sample_sub = pd.read_csv('../input/sample_submission.csv')\n",
    "class_names = list(sample_sub.columns[1:-1])\n",
    "del sample_sub;\n",
    "\n",
    "# Plot non-normalized confusion matrix\n",
    "plt.figure(figsize=(12,12))\n",
    "foo = plot_confusion_matrix(cnf_matrix, classes=class_names,normalize=True,\n",
    "                      title='Confusion matrix')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
